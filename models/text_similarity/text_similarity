# -*- coding: utf-8 -*-
"""text_similarity.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1fioIvCwtum2k2roB9X7ysg1Z2yzBz1sl
"""

# !pip install bert-embedding
# !pip install pandas

import os, re, io
import numpy as np
import requests
import pandas as pd
from gensim.parsing.preprocessing import remove_stopwords
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize
import spacy
nlp = spacy.load('en_core_web_sm')
from bert_embedding import BertEmbedding
from gensim.models import TfidfModel
from gensim.corpora import Dictionary
from sklearn.metrics.pairwise import cosine_similarity

## Data Preprocessing

"""
Blueprint of preprocessing and lemmatization for the text similarity
Args:
  data_df: Name of the Dataframe
  column_name : column name
Returns: 
  final_token : The list of the words
"""


class TextSimilarity():
    def __init__(self, data_df, column_name=None):
        """Init the Preprocessing"""
        self.data_df = data_df  
        if not column_name and type(column_name) == str:
            raise Exception("column name is mandatory. Make sure type is string format")
        self.column = column_name
        self.convert_lowercase()    
        self.applied_stopword = False
        self.processed_column_name = f"processed_{self.column}"
        
    def convert_lowercase(self):
        """text convert into lowercase"""
        ## fill empty values into empty
        self.data_df.fillna('',inplace=True)
        ## reduce all the columns to lowercase
        self.data_df = self.data_df.apply(lambda column: column.astype(str).str.lower(), axis=0)    

    def remove_question_no(self):
        """remove question no present in text"""
        ## remove question no        
        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'^\d+[.]',' ', row))    
        
    def remove_symbols(self):
        """remove unwanted characte"""
        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'[^A-Za-z0-9\s]', ' ', row))    

    def remove_stopwords(self):
        """remove stopwords and create a new column"""
        for idx, question in enumerate(self.data_df[self.column]):      
            self.data_df.loc[idx, self.processed_column_name] = remove_stopwords(question)        

    def apply_lemmatization(self, perform_stopword):
        """get the root words to reduce inflection of words"""
        lemmatizer = WordNetLemmatizer()    
        ## get the column name to perform lemma operation whether stopwords removed text or not
        if perform_stopword:
            column_name = self.processed_column_name
        else:
            column_name = self.column
        ## iterate every question, perform tokenize and lemma
        for idx, question in enumerate(self.data_df[column_name]):

            lemmatized_sentence = []
            ## use spacy for lemmatization
            doc = nlp(question.strip())
            for word in doc:       
                lemmatized_sentence.append(word.lemma_)      
                ## update to the same column
                self.data_df.loc[idx, self.processed_column_name] = " ".join(lemmatized_sentence)

    def run_all(self, perform_stopword = True):
        """Run all the methods as per the requirements"""
        self.remove_question_no()
        self.remove_symbols()
        if perform_stopword:
            self.remove_stopwords()
        self.apply_lemmatization(perform_stopword)    
        return self.data_df

df = pd.read_csv("COVID19_FAQ.csv")
df.head(10)

## pre-process training question data
question_answer = TextSimilarity(df.copy(), column_name="questions")
clean_df = question_answer.run_all()
clean_df.head(10)

test_query_questions = ["Am I considered a close contact if I was wearing a mask?",
"Is the virus that causes COVID-19 found in feces (stool)?",
"Can the COVID-19 virus spread through sewerage?"]

test_df = pd.DataFrame(test_query_questions, columns=["test_questions"])  

## pre-process testing QA data
question_answer = TextSimilarity(test_df, column_name="test_questions")
query_df = question_answer.run_all()
query_df

## get bert embeddings
def func_get_bert_embeddings(sentences):
    bert_embedding = BertEmbedding()
    return bert_embedding(sentences)

question_QA_bert_embeddings_list = func_get_bert_embeddings(clean_df["questions"].to_list())
query_QA_bert_embeddings_list = func_get_bert_embeddings(test_df["test_questions"].to_list())

## store QA bert embeddings in list
question_QA_bert_embeddings = []
for embeddings in question_QA_bert_embeddings_list:
    question_QA_bert_embeddings.append(embeddings[1])

## store query string bert embeddings in list
query_QA_bert_embeddings = []
for embeddings in query_QA_bert_embeddings_list:
    query_QA_bert_embeddings.append(embeddings[1])

## helps to retrieve similar question based of input vectors/embeddings for test query
def func_get_SimilarFAQ(train_question_vectors, test_question_vectors, train_df, train_column_name, test_df, test_column_name):
    similar_question_index = []
    final_similarity_score = []
    for test_index, test_vector in enumerate(test_question_vectors):
        sim, sim_Q_index = -1, -1
        for train_index, train_vector in enumerate(train_question_vectors):
            sim_score = cosine_similarity(train_vector, test_vector)[0][0]
            
            if sim < sim_score:
                print("sim_score", sim_score)
                # print("sim",sim)
                # break
                sim = sim_score
                final_similarity_score.append(sim_score)
                sim_Q_index = train_index
        
        print("final_similarity_score:", final_similarity_score.pop())    
        print(f"Query Question: {test_df[test_column_name].iloc[test_index]}")    
        print(f"Get Question: {train_df[train_column_name].iloc[sim_Q_index]}")
        print("\n")

func_get_SimilarFAQ(question_QA_bert_embeddings, query_QA_bert_embeddings, clean_df, "questions", query_df, "test_questions")

