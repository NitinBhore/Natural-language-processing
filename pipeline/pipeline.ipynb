{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnWosZ03mKTY"
      },
      "outputs": [],
      "source": [
        "# #auto-completion\n",
        "# !pip install jupyter_contrib_nbextensions\n",
        "# !jupyter contrib nbextension install - user\n",
        "# from jedi import settings\n",
        "# settings.case_insensitive_completion = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Install the kubeflow components.\n",
        "# USER_FLAG = \"--user\"\n",
        "# # Install ai platform and kfp\n",
        "# !pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
        "# !pip3 install {USER_FLAG} kfp --upgrade\n",
        "# !pip install google_cloud_pipeline_components"
      ],
      "metadata": {
        "id": "RrBKu3CemTGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Enable the APIs if they are not enabled\n",
        "# !gcloud services enable compute.googleapis.com         \\\n",
        "#                        containerregistry.googleapis.com  \\\n",
        "#                        aiplatform.googleapis.com  \\\n",
        "#                        cloudbuild.googleapis.com \\\n",
        "#                        cloudfunctions.googleapis.com"
      ],
      "metadata": {
        "id": "xEhC_jcxmTHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###Import the libraries\n",
        "from typing import NamedTuple\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2.dsl import (Artifact,\n",
        "                        Dataset,\n",
        "                        Input,\n",
        "                        Model,\n",
        "                        Output,\n",
        "                        Metrics,\n",
        "                        ClassificationMetrics,\n",
        "                        component, \n",
        "                        OutputPath, \n",
        "                        InputPath)\n",
        "\n",
        "from kfp.v2 import compiler\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import pipeline_jobs\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip"
      ],
      "metadata": {
        "id": "fXnepaiBmTLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###Set up the global variables\n",
        "\n",
        "# PATH=%env PATH\n",
        "# %env PATH={PATH}:/home/jupyter/.local/bin\n",
        "# REGION=\"europe-west1\"\n",
        "\n",
        "# # Get projet name\n",
        "# shell_output=!gcloud config get-value project 2> /dev/null\n",
        "# PROJECT_ID=shell_output[0]\n",
        "\n",
        "# # Set bucket name\n",
        "# BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket-smsquality\n",
        "\n",
        "# # Create bucket\n",
        "# PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_wine/\"\n",
        "# PIPELINE_ROOT\n",
        "\n",
        "# USER_FLAG = \"--user\"\n",
        "# #!gcloud auth login if needed"
      ],
      "metadata": {
        "id": "Z_6YwzkCmTM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Install a virtual env and use it inside the JupyterLab\n",
        "# #Install virtual env \n",
        "# python -m pip install --user virtualenv\n",
        "# echo \"create env\"\n",
        "# python -m venv vertex_venv\n",
        "# #Add kernel to jupyter\n",
        "# echo \"Add kernel to jupyter\"\n",
        "# ipython kernel install --name \"vertex_env\" --user"
      ],
      "metadata": {
        "id": "dmKgWFyHmTQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nP4S72BTuVE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Load the data\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"numpy\",\"pandas\", \"sklearn\", \"tensorflow\", \"matplotlib\", \"seaborn\"],\n",
        "    base_image=\"python:3.9\",\n",
        "    output_component_file=\"get_data.yaml\"\n",
        ")\n",
        "\n",
        "def get_data(\n",
        "    url: str,\n",
        "    dataset_train: Output[Dataset],\n",
        "    dataset_test: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import numpy as np\n",
        "    import pandas as pd # pylint: disable=unused-import\n",
        "    from sklearn.model_selection import train_test_split as tts\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub # pylint: disable=unused-import\n",
        "    import tensorflow_text as text # pylint: disable=unused-import\n",
        "    from sklearn.metrics import classification_report  # pylint: disable=ungrouped-imports\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from matplotlib import pyplot as plt\n",
        "    import seaborn as sn\n",
        "\n",
        "    df_spam = pd.read_csv(url)\n",
        "    \n",
        "    # Create the label collumn\n",
        "    df_spam['target']=df_spam['Class'].apply(lambda x: 1 if x=='spam' else 0)\n",
        "    df_spam = df_spam.drop(['Class'], axis=1)\n",
        "  \n",
        "    train, test = tts(df_spam, test_size=0.3)\n",
        "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
        "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "fV8Ai-ppmTSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Train the model\n",
        "\n",
        "@component(\n",
        "    packages_to_install = [\"numpy\",\"pandas\", \"sklearn\", \"tensorflow\", \"matplotlib\", \"seaborn\"], \n",
        "    base_image=\"python:3.9\",\n",
        ")\n",
        "def train_sms(dataset: Input[Dataset], model: Output[Model]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import pickle\n",
        "    import numpy as np\n",
        "    import pandas as pd # pylint: disable=unused-import\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub # pylint: disable=unused-import\n",
        "    import tensorflow_text as text # pylint: disable=unused-import\n",
        "    from sklearn.metrics import classification_report  # pylint: disable=ungrouped-imports\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from matplotlib import pyplot as plt\n",
        "    import seaborn as sn\n",
        "    bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "    bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "    data = pd.read_csv(dataset.path+\".csv\")\n",
        "    ####\n",
        "\n",
        "    # Bert layers\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessed_text = bert_preprocess(text_input)\n",
        "    outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "    # Neural network layers\n",
        "    lay = tf.keras.layers.Dense(64, activation='relu', name=\"dense1\")(outputs['pooled_output'])\n",
        "    lay = tf.keras.layers.Dropout(0.2, name=\"dropout1\")(lay)\n",
        "    lay = tf.keras.layers.Dense(32, activation='relu', name=\"dense2\")(lay)\n",
        "    lay = tf.keras.layers.Dropout(0.2, name=\"dropout\")(lay)\n",
        "    lay = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output2\")(lay)\n",
        "\n",
        "    # Use inputs and outputs to construct a final model\n",
        "    model = tf.keras.Model(inputs=[text_input], outputs=[lay])\n",
        "\n",
        "    # print summary\n",
        "    model.summary()\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    # train the model\n",
        "    model_rf = model.fit(data['sms'], data['target'], epochs=2 )\n",
        "\n",
        "    # #####\n",
        "    # model_rf = RandomForestClassifier(n_estimators=10)\n",
        "    # model_rf.fit(\n",
        "    #     data.drop(columns=[\"target\"]),\n",
        "    #     data.target,\n",
        "    # )\n",
        "\n",
        "    model.metadata[\"framework\"] = \"RF\"\n",
        "    file_name = model.path + f\".pkl\"\n",
        "    with open(file_name, 'wb') as file:  \n",
        "        pickle.dump(model_rf, file)"
      ],
      "metadata": {
        "id": "E2owv-xImTWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Evaluate the model\n",
        "\n",
        "@component(\n",
        "    packages_to_install = [\"numpy\", \"pandas\", \"sklearn\", \"tensorflow\", \"matplotlib\", \"seaborn\"], base_image=\"python:3.9\")\n",
        "def smsquality_evaluation(\n",
        "    test_set:  Input[Dataset],\n",
        "    rf_smsquality_model: Input[Model],\n",
        "    thresholds_dict_str: str,\n",
        "    metrics: Output[ClassificationMetrics],\n",
        "    kpi: Output[Metrics]\n",
        ") -> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import logging \n",
        "    import pickle\n",
        "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, classification_report\n",
        "    import json\n",
        "    import typing\n",
        "    import numpy as np    \n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    import tensorflow_text as text    \n",
        "    from matplotlib import pyplot as plt\n",
        "    import seaborn as sn\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "       \n",
        "    def threshold_check(val1, val2):\n",
        "        cond = \"false\"\n",
        "        if val1 >= val2 :\n",
        "            cond = \"true\"\n",
        "        return cond\n",
        "\n",
        "    data = pd.read_csv(test_set.path+\".csv\")\n",
        "    ####\n",
        "\n",
        "    file_name = rf_smsquality_model.path + \".pkl\"\n",
        "    with open(file_name, 'rb') as file:  \n",
        "        model = pickle.load(file)\n",
        "    \n",
        "    y_test = data.drop(columns=[\"target\"])\n",
        "    y_target=data.target\n",
        "    y_pred = model.predict(y_test)\n",
        "    \n",
        "    y_scores =  model.predict_proba(data.drop(columns=[\"target\"]))[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(\n",
        "         y_true=data.target.to_numpy(), y_score=y_scores, pos_label=True\n",
        "    )\n",
        "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())  \n",
        "    \n",
        "    metrics.log_confusion_matrix(\n",
        "       [\"False\", \"True\"],\n",
        "       confusion_matrix(\n",
        "           data.target, y_pred\n",
        "       ).tolist(), \n",
        "    )\n",
        "    \n",
        "    accuracy = accuracy_score(data.target, y_pred.round())\n",
        "    thresholds_dict = json.loads(thresholds_dict_str)\n",
        "    rf_smsquality_model.metadata[\"accuracy\"] = float(accuracy)\n",
        "    kpi.log_metric(\"accuracy\", float(accuracy))\n",
        "    deploy = threshold_check(float(accuracy), int(thresholds_dict['roc']))\n",
        "\n",
        "    deploy = model\n",
        "    return deploy"
      ],
      "metadata": {
        "id": "kROzLWDmmTYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Deploy the model\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform\", \"scikit-learn==1.0.0\",  \"kfp\"],\n",
        "    base_image=\"python:3.9\",\n",
        "    output_component_file=\"model_smsquality_coponent.yml\"\n",
        ")\n",
        "def deploy_smsequality(\n",
        "    model: Input[Model],\n",
        "    project: str,\n",
        "    region: str,\n",
        "    serving_container_image_uri : str, \n",
        "    vertex_endpoint: Output[Artifact],\n",
        "    vertex_model: Output[Model]\n",
        "):\n",
        "    from google.cloud import aiplatform\n",
        "    aiplatform.init(project=project, location=region)\n",
        "\n",
        "    DISPLAY_NAME  = \"smsquality\"\n",
        "    MODEL_NAME = \"smsquality-rf\"\n",
        "    ENDPOINT_NAME = \"smsquality_endpoint\"\n",
        "    \n",
        "    def create_endpoint():\n",
        "        endpoints = aiplatform.Endpoint.list(\n",
        "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
        "        order_by='create_time desc',\n",
        "        project=project, \n",
        "        location=region,\n",
        "        )\n",
        "        if len(endpoints) > 0:\n",
        "            endpoint = endpoints[0]  # most recently created\n",
        "        else:\n",
        "            endpoint = aiplatform.Endpoint.create(\n",
        "            display_name=ENDPOINT_NAME, project=project, location=region\n",
        "        )\n",
        "    endpoint = create_endpoint()   \n",
        "    \n",
        "    \n",
        "    #Import a model programmatically\n",
        "    model_upload = aiplatform.Model.upload(\n",
        "        display_name = DISPLAY_NAME, \n",
        "        artifact_uri = model.uri.replace(\"model\", \"\"),\n",
        "        serving_container_image_uri =  serving_container_image_uri,\n",
        "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
        "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
        "        serving_container_environment_variables={\n",
        "        \"MODEL_NAME\": MODEL_NAME,\n",
        "    },       \n",
        "    )\n",
        "    model_deploy = model_upload.deploy(\n",
        "        machine_type=\"n1-standard-4\", \n",
        "        endpoint=endpoint,\n",
        "        traffic_split={\"0\": 100},\n",
        "        deployed_model_display_name=DISPLAY_NAME,\n",
        "    )\n",
        "\n",
        "    # Save data to the output params\n",
        "    vertex_model.uri = model_deploy.resource_name"
      ],
      "metadata": {
        "id": "6FOfnq6DmTcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Create the pipeline\n",
        "\n",
        "from datetime import datetime\n",
        "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "DISPLAY_NAME = 'pipeline-smsquality-job{}'.format(TIMESTAMP)\n",
        "\n",
        "\n",
        "####\n",
        "@dsl.pipeline(\n",
        "    # Default pipeline root. You can override it when submitting the pipeline.\n",
        "    pipeline_root= PIPELINE_ROOT,\n",
        "    # A name for the pipeline. Use to determine the pipeline Context.\n",
        "    name= pipeline-smsquality\n",
        "    \n",
        ")\n",
        "def pipeline(\n",
        "    url: str =  \"SMSCollection.csv\"  #storage path \n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION, \n",
        "    display_name: str = DISPLAY_NAME,\n",
        "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
        "    thresholds_dict_str: str = '{\"roc\":0.8}',\n",
        "    serving_container_image_uri: str = \"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
        "    ):\n",
        "    \n",
        "    data_op = get_data(url)\n",
        "    train_model_op = train_sms(data_op.outputs[\"dataset_train\"])\n",
        "    model_evaluation_op = smsquality_evaluation(\n",
        "        test_set=data_op.outputs[\"dataset_test\"],\n",
        "        rf_smsquality_model=train_model_op.outputs[\"model\"],\n",
        "        thresholds_dict_str = thresholds_dict_str, # I deploy the model anly if the model performance is above the threshold\n",
        "    )\n",
        "    \n",
        "    with dsl.Condition(\n",
        "        model_evaluation_op.outputs[\"deploy\"]==\"true\",\n",
        "        name=\"deploy-smsquality\",\n",
        "    ):\n",
        "           \n",
        "        deploy_model_op = deploy_smsequality(\n",
        "        model=train_model_op.outputs['model'],\n",
        "        project=project,\n",
        "        region=region, \n",
        "        serving_container_image_uri = serving_container_image_uri,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "bxlNLgF3mTd6",
        "outputId": "95a5e5b6-1384-4d9d-8e45-713bf1f98534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e64a4b498896>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    project: str = PROJECT_ID,\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Run the pipeline\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path='ml_smsquality.json')\n",
        "\n"
      ],
      "metadata": {
        "id": "bfAocuBymTh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create a run using the job spec file previously generated\n",
        "start_pipeline = pipeline_jobs.PipelineJob(\n",
        "    display_name=\"smsquality-pipeline\",\n",
        "    template_path=\"ml_smsquality.json\",\n",
        "    enable_caching=True,\n",
        "    location=REGION,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bZK3Y74FmTjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Run the pipeline\n",
        "\n",
        "start_pipeline.run()"
      ],
      "metadata": {
        "id": "Z6wRHAyjmTnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nc2IzZdqmTog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upSCHQvFmT-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jzv6SXtGmUC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7wZ1fvTmUFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "93T86V6gmUIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}