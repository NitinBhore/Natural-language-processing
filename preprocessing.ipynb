{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StnMojUscDRB",
        "outputId": "072adf28-60a9-4a9f-84b4-a32796500f4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['find',\n",
              " 'alternative',\n",
              " 'pagerduty',\n",
              " 'shopwebly',\n",
              " 'website',\n",
              " 'compare',\n",
              " 'price',\n",
              " 'find',\n",
              " 'compare',\n",
              " 'alternative',\n",
              " 'pagerduty',\n",
              " 'online',\n",
              " 'save',\n",
              " 'shopwebly',\n",
              " 'product',\n",
              " 'quick',\n",
              " 'result',\n",
              " 'easy',\n",
              " 'access',\n",
              " 'compare',\n",
              " 'product',\n",
              " 'search',\n",
              " 'discover',\n",
              " '  ']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "\"\"\"\n",
        "Blueprint of preprocessing and lemmatization for the Natural language \n",
        "processing\n",
        "\n",
        "Args:\n",
        "  doc: List of the sentences\n",
        "\n",
        "Returns: \n",
        "  final_token : The list of the words\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Preprocessing():\n",
        "  def __init__(self, doc):\n",
        "    \"\"\"Inits the Preprocessing\"\"\"\n",
        "    self.doc = doc\n",
        "  \n",
        "  def cleanData(self, doc):\n",
        "    \"\"\"Clean the data by removing stop words punctuvation\"\"\"\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = doc.lower()\n",
        "    doc = nlp(doc)\n",
        "    tokens = [tokens.lower_ for tokens in doc]\n",
        "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
        "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
        "    # print(tokens)\n",
        "    return tokens\n",
        "\n",
        "  def lemmatization(self, tokens):\n",
        "    \"\"\"Lemmatiz the data\"\"\"\n",
        "      # self.tokens = tokens\n",
        "      final_token = [token.lemma_ for token in tokens]\n",
        "      # print(\" \".join(final_token))\n",
        "      return final_token #\" \".join(final_token)\n",
        " \n",
        "  def run_all(self):\n",
        "    \"\"\"Run all the methods as per the requirments\"\"\"\n",
        "    tokens = self.cleanData(self.doc)\n",
        "    final_token = self.lemmatization(tokens)\n",
        "    return final_token\n",
        "\n",
        "doc = (\"Find Alternatives To Pagerduty at Shopwebly, the Website to Compare Prices! Find and Compare Alternatives To Pagerduty Online. Save Now at Shopwebly! Many Products. Quick Results. Easy Access. Compare Products. Search and Discover   \")\n",
        "preprocessing = Preprocessing(doc)\n",
        "preprocessing.run_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WiKvM5ZEAIl0",
        "outputId": "4582b635-be6f-4d64-b0c5-e67237c645be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Collecting tensorflow<2.11,>=2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.3.0)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 74.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (57.4.0)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.48.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (3.17.3)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (0.26.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow_text) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.11,>=2.10.0->tensorflow_text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed gast-0.4.0 keras-2.10.0 tensorboard-2.10.0 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-text-2.10.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "v56YLQwi0yCG",
        "outputId": "27d4123f-43ae-4884-a3fb-69d1a79f635c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "131/131 [==============================] - 51s 314ms/step - loss: 0.3252 - accuracy: 0.8641 - precision: 0.4231 - recall: 0.0393\n",
            "Epoch 2/2\n",
            "131/131 [==============================] - 42s 323ms/step - loss: 0.2328 - accuracy: 0.8985 - precision: 0.8434 - recall: 0.2982\n",
            "44/44 [==============================] - 14s 313ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95      1206\n",
            "           1       0.90      0.35      0.51       187\n",
            "\n",
            "    accuracy                           0.91      1393\n",
            "   macro avg       0.91      0.67      0.73      1393\n",
            "weighted avg       0.91      0.91      0.89      1393\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "######   BERT model, ALBERT - A Lite BERT model  and  RoBERTa - Robustly Optimized BERT Pretraining Approach   ############\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Blueprint of Bidirectional Encoder Representations from Transformers. BERT is a transformer-based machine learning technique for \\\n",
        "natural language processing.\n",
        "\n",
        "Args:\n",
        "  bert_preprocess : preprocess URL link\n",
        "  bert_encoder : encoder URL link\n",
        "  df_balanced : dataframe name\n",
        "  text_column : text column name\n",
        "  label_column : target column name\n",
        "  epochs : number of epochs\n",
        "\n",
        "Returns: \n",
        "  model : BERT model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class BERT():\n",
        "  def __init__(self, bert_preprocess, bert_encoder, df_balanced, text_column, label_column, epochs):\n",
        "    \"\"\"Inits the BERT\"\"\"\n",
        "    self.bert_preprocess = bert_preprocess\n",
        "    self.bert_encoder = bert_encoder\n",
        "    self.df_balanced = df_balanced\n",
        "    self.text_column = text_column\n",
        "    self.label_column = label_column\n",
        "    self.epochs = epochs\n",
        "\n",
        "  \n",
        "  def func_bert(self, bert_preprocess, bert_encoder, df_balanced, text_column, label_column, epochs):\n",
        "    \"\"\"Perform the Bidirectional Encoder Representations from Transformers\"\"\"\n",
        "    # Split it into training and test data set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df_balanced[text_column], df_balanced[label_column], stratify=df_balanced['spam'])\n",
        "\n",
        "\n",
        "    # Bert layers\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessed_text = bert_preprocess(text_input)\n",
        "    outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "    # Neural network layers\n",
        "    l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
        "    l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "\n",
        "    # Use inputs and outputs to construct a final model\n",
        "    model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
        "\n",
        "    #model.summary()\n",
        "    METRICS = [\n",
        "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "          tf.keras.metrics.Precision(name='precision'),\n",
        "          tf.keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=METRICS)\n",
        "\n",
        "    # train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs)\n",
        "    \n",
        "    y_predicted = model.predict(X_test)\n",
        "    y_predicted = y_predicted.flatten()\n",
        "\n",
        "    y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "\n",
        "    print(classification_report(y_test, y_predicted)) \n",
        "    cm = confusion_matrix(y_test, y_predicted)\n",
        "    sn.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Truth')\n",
        "    return model\n",
        " \n",
        "  def run_all(self):\n",
        "    \"\"\"Run all the methods as per the requirements\"\"\"\n",
        "    model = self.func_bert(self.bert_preprocess, self.bert_encoder, self.df_balanced, self.text_column, self.label_column, self.epochs)\n",
        "    return model\n",
        "\n",
        "# import BERT model\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "# #import ALBERT - A Lite BERT model\n",
        "# bert_preprocess = hub.KerasLayer(\"http://tfhub.dev/tensorflow/albert_en_preprocess/3\")\n",
        "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n",
        "\n",
        "# #import RoBERTa - Robustly Optimized BERT Pretraining Approach\n",
        "# bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1\")\n",
        "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"SMSCollection.csv\")\n",
        "df.head()\n",
        "\n",
        "df_balanced = df\n",
        "df_balanced['spam']=df_balanced['Class'].apply(lambda x: 1 if x=='spam' else 0)\n",
        "df_balanced.head()\n",
        "\n",
        "#class Bert \n",
        "bertExe = BERT(bert_preprocess,  bert_encoder, df_balanced, 'sms', 'spam', 2)\n",
        "model = bertExe.run_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP0tTXtH7GKB"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Bert\":{\n",
        "    \"preprocess\" : \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
        "    \"encoder\" : \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
        "    },\n",
        " \n",
        " \"Albert\":{\n",
        "    \"preprocess\" : \"http://tfhub.dev/tensorflow/albert_en_preprocess/3\",\n",
        "    \"encoder\" : \"https://tfhub.dev/tensorflow/albert_en_base/3\"\n",
        "    },\n",
        " \n",
        " \"RoBERTa\":{\n",
        "    \"preprocess\" : \"http://tfhub.dev/tensorflow/albert_en_preprocess/3\",\n",
        "    \"encoder\" : \"https://tfhub.dev/tensorflow/albert_en_base/3\"\n",
        "   }\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOCDi801z8y7",
        "outputId": "94d6f005-9ee7-44b0-b75b-6a615595ee3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "100%|██████████| 100/100 [00:00<00:00, 6149.37it/s]\n",
            "100%|██████████| 608/608 [00:00<00:00, 167222.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 8s 624ms/step - loss: 0.7139 - acc: 0.2125 - val_loss: 0.7130 - val_acc: 0.0500\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.7045 - acc: 0.2125 - val_loss: 0.7063 - val_acc: 0.1500\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.7022 - acc: 0.3500 - val_loss: 0.7004 - val_acc: 0.2500\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6968 - acc: 0.4250 - val_loss: 0.6954 - val_acc: 0.4000\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.6938 - acc: 0.5375 - val_loss: 0.6910 - val_acc: 0.6000\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.6892 - acc: 0.6000 - val_loss: 0.6872 - val_acc: 0.7500\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.6884 - acc: 0.6125 - val_loss: 0.6837 - val_acc: 0.9000\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.6883 - acc: 0.6500 - val_loss: 0.6806 - val_acc: 0.9000\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.6857 - acc: 0.7250 - val_loss: 0.6779 - val_acc: 0.9000\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.6831 - acc: 0.6750 - val_loss: 0.6752 - val_acc: 0.9000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95        18\n",
            "           1       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.90        20\n",
            "   macro avg       0.45      0.50      0.47        20\n",
            "weighted avg       0.81      0.90      0.85        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "##############    GloVe-Contextualized Vectors uisng LSTM    #######################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,RNN, LSTM,Dense,SpatialDropout1D\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\"\"\"\n",
        "GloVe stands for Global Vectors for word representation. It is an unsupervised \n",
        "learning algorithm. Global Vectors  generate word embeddings by aggregating \n",
        "global word co-occurrence matrices from a given corpus.\n",
        "\n",
        "Args:\n",
        "  embedding_dict : Embedding dictionary\n",
        "  df : Dataframe name\n",
        "  column  : Text column name\n",
        "\n",
        "Returns: \n",
        "  model : GloVe-Contextualized Vectors with LSTM model\n",
        "\n",
        "\"\"\"\n",
        "class GloVe():\n",
        "  def __init__(self, embedding_dict, df, sms):\n",
        "    \"\"\"Inits the GloVe\"\"\"\n",
        "    self.embedding_dict = embedding_dict\n",
        "    self.df = df\n",
        "    self.sms = sms\n",
        "\n",
        "  # clean text\n",
        "  def clean_text(self, text):\n",
        "    \"\"\"Clean the text\"\"\"\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)  \n",
        "    text = text.lower()  \n",
        "    text = text.split(' ')      \n",
        "    text = [w for w in text if not w in set(stopwords.words('english'))] \n",
        "    text = ' '.join(text)            \n",
        "    return text\n",
        "\n",
        "  # create the corpus GloVe \n",
        "  def create_corpus(self, df):\n",
        "      \"\"\"Create the corpus\"\"\"\n",
        "      corpus=[]\n",
        "      for tweet in tqdm(df['clean_sms']):\n",
        "          words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
        "          corpus.append(words)\n",
        "      return corpus\n",
        "  \n",
        "  def run_all(self):\n",
        "    \"\"\"Run all the methods as per the requirements\"\"\"\n",
        "    df['clean_sms'] = df['sms'].apply(lambda x : self.clean_text(x))\n",
        "\n",
        "    # padding\n",
        "    MAX_LEN=10\n",
        "    tokenizer_obj=Tokenizer()\n",
        "\n",
        "    corpus=self.create_corpus(df)\n",
        "    tokenizer_obj.fit_on_texts(corpus)\n",
        "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "    email_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
        "\n",
        "    word_index=tokenizer_obj.word_index\n",
        "\n",
        "    # Embedding\n",
        "    num_words=len(word_index)+1\n",
        "    embedding_matrix=np.zeros((num_words,50))\n",
        "\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        \n",
        "        emb_vec=embedding_dict.get(word)\n",
        "        if emb_vec is not None:\n",
        "            embedding_matrix[i]=emb_vec\n",
        "\n",
        "    # Dataset split\n",
        "    X_train,X_val, y_train, y_val = train_test_split(email_pad,df.spam, test_size=.2, random_state=2)\n",
        "\n",
        "    # Create Model.\n",
        "    model=Sequential()\n",
        "\n",
        "    embedding_layer=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n",
        "                      input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(16))\n",
        "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimzer=Adam(learning_rate=0.0001)\n",
        "    model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['acc'])\n",
        "    # model.summary()\n",
        "\n",
        "    #Fitting The Model\n",
        "    history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_val,y_val),verbose=1)\n",
        "\n",
        "    y_predicted = model.predict(X_val)\n",
        "    y_predicted = y_predicted.flatten()\n",
        "\n",
        "    y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "\n",
        "    print(classification_report(y_val, y_predicted))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "embedding_dict={}\n",
        "with open('glove.6B.50d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "df = pd.read_csv(\"SMSCollection.csv\")\n",
        "df['spam']=df['Class'].replace({'ham':0,'spam':1})\n",
        "df = df.head(100)\n",
        "df.head()\n",
        "\n",
        "\n",
        "GloVeExe = GloVe(embedding_dict, df, 'sms')\n",
        "model = GloVeExe.run_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVW7ILjAjYdN"
      },
      "outputs": [],
      "source": [
        "##############    GloVe-Contextualized Vectors uisng RNN    #######################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,RNN, SimpleRNN,Dense,SpatialDropout1D\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "class GloVe():\n",
        "  \"\"\"\n",
        "  GloVe stands for Global Vectors for word representation. It is an unsupervised \n",
        "  learning algorithm. Global Vectors  generate word embeddings by aggregating \n",
        "  global word co-occurrence matrices from a given corpus.\n",
        "\n",
        "  Args:\n",
        "  embedding_dict : Embedding dictionary\n",
        "  df : Dataframe name\n",
        "  column  : Text column name\n",
        "\n",
        "  Returns: \n",
        "    model : GloVe-Contextualized Vectors with SimpleRNN model\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dict, df, sms):\n",
        "    \"\"\" Inits the Preprocessing \"\"\"\n",
        "    self.embedding_dict = embedding_dict\n",
        "    self.df = df\n",
        "    self.sms = sms\n",
        "\n",
        "  # clean text\n",
        "  def clean_text(self, text):\n",
        "    \"\"\"Clean the text\"\"\"\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)  \n",
        "    text = text.lower()  \n",
        "    text = text.split(' ')      \n",
        "    text = [w for w in text if not w in set(stopwords.words('english'))] \n",
        "    text = ' '.join(text)            \n",
        "    return text\n",
        "\n",
        "  # create the corpus GloVe \n",
        "  def create_corpus(self, df):\n",
        "      \"\"\" create the corpus GloVe \"\"\"\n",
        "      corpus=[]\n",
        "      for tweet in tqdm(df['clean_sms']):\n",
        "          words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
        "          corpus.append(words)\n",
        "      return corpus\n",
        "  \n",
        "  def run_all(self):\n",
        "    \"\"\" Run all the methods as per the requirements \"\"\"\n",
        "    df['clean_sms'] = df['sms'].apply(lambda x : self.clean_text(x))\n",
        "\n",
        "    # padding\n",
        "    MAX_LEN=10\n",
        "    tokenizer_obj=Tokenizer()\n",
        "\n",
        "    corpus=self.create_corpus(df)\n",
        "    tokenizer_obj.fit_on_texts(corpus)\n",
        "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "\n",
        "    email_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
        "\n",
        "    word_index=tokenizer_obj.word_index\n",
        "\n",
        "    # Embedding\n",
        "    num_words=len(word_index)+1\n",
        "    embedding_matrix=np.zeros((num_words,50))\n",
        "\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        \n",
        "        emb_vec=embedding_dict.get(word)\n",
        "        if emb_vec is not None:\n",
        "            embedding_matrix[i]=emb_vec\n",
        "\n",
        "    # Dataset split\n",
        "    X_train,X_val, y_train, y_val = train_test_split(email_pad,df.spam, test_size=.2, random_state=2)\n",
        "\n",
        "    # Create Model.\n",
        "    model=Sequential()\n",
        "\n",
        "    embedding_layer=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n",
        "                      input_length=MAX_LEN,trainable=False)\n",
        "\n",
        "    model.add(embedding_layer)\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
        "    model.add(tf.keras.layers.SimpleRNN(32,return_sequences=True))\n",
        "    model.add(tf.keras.layers.SimpleRNN(16))\n",
        "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    optimzer=Adam(learning_rate=0.0001)\n",
        "    model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['acc'])\n",
        "    # model.summary()\n",
        "\n",
        "    #Fitting The Model\n",
        "    history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_val,y_val),verbose=1)\n",
        "\n",
        "    y_predicted = model.predict(X_val)\n",
        "    y_predicted = y_predicted.flatten()\n",
        "\n",
        "    y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "\n",
        "    print(classification_report(y_val, y_predicted))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "embedding_dict={}\n",
        "with open('glove.6B.50d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vectors=np.asarray(values[1:],'float32')\n",
        "        embedding_dict[word]=vectors\n",
        "f.close()\n",
        "\n",
        "df = pd.read_csv(\"SMSCollection.csv\")\n",
        "df['spam']=df['Class'].replace({'ham':0,'spam':1})\n",
        "df = df.head(100)\n",
        "df.head()\n",
        "\n",
        "\n",
        "GloVeExe = GloVe(embedding_dict, df, 'sms')\n",
        "model = GloVeExe.run_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWe7DFxBKeaS",
        "outputId": "a167faec-ead4-40d3-b959-7a81bdb85d76"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.09527239, -0.30841684,  0.1721609 ,  0.10372017,  0.17012785,\n",
              "       -0.05623797,  0.11417595, -0.18692602,  0.26557827, -0.11793958,\n",
              "       -0.23201188,  0.24393639,  0.02137129,  0.00878783, -0.14745054,\n",
              "       -0.19472069,  0.1455558 , -0.06151609,  0.2699784 , -0.24014671,\n",
              "       -0.11460411,  0.15342708,  0.12222952,  0.16881798, -0.04026898,\n",
              "        0.26504946, -0.06621742, -0.42141995, -0.10061757, -0.0265855 ,\n",
              "       -0.25427124, -0.36587232,  0.01036259, -0.17440367,  0.02230715,\n",
              "       -0.10369407,  0.06219948,  0.13727365,  0.06151349,  0.17567234],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#######  Doc2Vec  #########\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\"\"\"\n",
        "Doc2vec is an NLP tool for representing documents as a vector and is a generalizing of the word2vec method.\n",
        "Args:\n",
        "  doc: List of the sentences\n",
        "\n",
        "Returns: \n",
        "  model : Doc2Vec Model\n",
        "\n",
        "\"\"\"\n",
        "class Doc2Vec():\n",
        "  def __init__(self, doc):\n",
        "    \"\"\"Inits the Preprocessing\"\"\"\n",
        "    self.doc = doc\n",
        "  \n",
        "  ######\n",
        "  def tagged_document(self, list_of_list_of_words):\n",
        "    \"\"\"tagged the documents\"\"\"\n",
        "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
        "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
        "    # data_for_training = list(tagged_document(doc))\n",
        "\n",
        "    # return data_for_training\n",
        "\n",
        "  def doc2vec_model_train(self,data_for_training):\n",
        "    \"\"\"doc2vec model\"\"\"\n",
        "    # Initialise the Model\n",
        "    model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
        "\n",
        "    # build the vocabulary\n",
        "    model.build_vocab(data_for_training)\n",
        "\n",
        "    # train the Doc2Vec model\n",
        "    model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "    return model\n",
        " \n",
        "  def run_all(self):\n",
        "    \"\"\"Run all the methods as per the requirements\"\"\"\n",
        "    # data_for_training = self.tagged_document(self.doc)\n",
        "    model = self.doc2vec_model_train(list(self.tagged_document(self.doc)))\n",
        "    return model\n",
        "\n",
        "\n",
        "#Download the Dataset\n",
        "dataset = api.load(\"text8\")\n",
        "data = [d for d in dataset]\n",
        "\n",
        "\n",
        "doc2vec = Doc2Vec(data)\n",
        "model = doc2vec.run_all()\n",
        "model.infer_vector(['violent', 'means', 'to', 'destroy', 'the','organization'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Z96YhJALEi"
      },
      "outputs": [],
      "source": [
        "###########   Encoder-Decoder   ##############\n",
        "\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the input features\n",
        "\n",
        "def load_train(file_name):\n",
        "    pd.set_option('display.max_colwidth',None)\n",
        "    train =pd.read_csv(file_name) \n",
        "    train=train.dropna()\n",
        "    return train\n",
        "    \n",
        "train_df = load_train(\"SMSCollection.csv\")\n",
        "# train_df = train_df.head(100)\n",
        "train_df['spam']=train_df['Class'].apply(lambda x: 1 if x=='spam' else 0)\n",
        "X = train_df['sms'] # input\n",
        "y = train_df[['spam']].values # target /label\n",
        "\n",
        "sentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=11)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_val = tokenizer.texts_to_sequences(sentences_val)\n",
        "\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1 # (in case of pre-trained embeddings it's +2)                         \n",
        "maxlen = 131 # sentence length\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "\n",
        "maxlen = 131\n",
        "max_features = 50000\n",
        "embed_size = 131\n",
        "\n",
        "encoder_inp   = Input(shape=(maxlen,))\n",
        "encoder_embed = Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(encoder_inp)\n",
        "encoder_lstm_cell = LSTM(60,return_state='True')\n",
        "encoder_output,encoder_state_h,encoder_state_c = encoder_lstm_cell(encoder_embed)\n",
        "#Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n",
        "decoder_inp   = Input(shape=(maxlen,))\n",
        "decoder_embed = Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(decoder_inp)\n",
        "decoder_lstm_cell = LSTM(60,return_sequences='True',return_state=True)\n",
        "decoder_output,decoder_state_h,decoder_state_c = decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_h,encoder_state_c])\n",
        "decoder_dense_cell1 = Dense(16,activation='relu')\n",
        "decoder_d_output    = decoder_dense_cell1(decoder_output)\n",
        "decoder_dense_cell2 = Dense(1,activation='sigmoid')\n",
        "decoder_output = decoder_dense_cell2(decoder_d_output)\n",
        "model = Model([encoder_inp,decoder_inp],decoder_output) \n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#model.summary()\n",
        "history = model.fit([X_train,X_train],y_train,batch_size=1024,epochs=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CDKGF9jsKecO",
        "outputId": "aa0b8e05-1caa-490a-9edb-f2a164b52ad0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-59ccdf1c-4831-42f4-bc31-f27abb83e3d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amazing</th>\n",
              "      <th>best</th>\n",
              "      <th>children</th>\n",
              "      <th>game</th>\n",
              "      <th>great</th>\n",
              "      <th>man</th>\n",
              "      <th>sat</th>\n",
              "      <th>series</th>\n",
              "      <th>thrones</th>\n",
              "      <th>tv</th>\n",
              "      <th>walk</th>\n",
              "      <th>went</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59ccdf1c-4831-42f4-bc31-f27abb83e3d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59ccdf1c-4831-42f4-bc31-f27abb83e3d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59ccdf1c-4831-42f4-bc31-f27abb83e3d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   amazing  best  children  game  great  man  sat  series  thrones  tv  walk  \\\n",
              "0        0     0         0     0      0    1    0       0        0   0     1   \n",
              "1        0     0         1     0      0    0    1       0        0   0     0   \n",
              "2        1     0         0     1      0    0    0       1        1   1     0   \n",
              "3        0     1         0     1      0    0    0       1        1   1     0   \n",
              "4        0     0         0     1      1    0    0       0        1   0     0   \n",
              "\n",
              "   went  \n",
              "0     1  \n",
              "1     0  \n",
              "2     0  \n",
              "3     0  \n",
              "4     0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#####   Bag Of Words    ##############\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def func_baf_of_words(docList):\n",
        "  vectorizer = CountVectorizer(stop_words='english')\n",
        "  X = vectorizer.fit_transform(docList)\n",
        "  df_bow = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
        "  return df_bow\n",
        "\n",
        "docList = ['the man went out for a walk', 'the children sat around the fire', 'Game of Thrones is an amazing tv series!', 'Game of Thrones is the best tv series!', 'Game of Thrones is so great']\n",
        "result = func_baf_of_words(docList)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI6BXcgqKehL"
      },
      "outputs": [],
      "source": [
        "#####   Bag Of Words with N_grams   #############\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def func_baf_of_words(docList, ngram_range=(2,2)):\n",
        "  vectorizer = CountVectorizer(stop_words='english', ngram_range= ngram_range)\n",
        "  X = vectorizer.fit_transform(docList)\n",
        "  df_bow = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
        "  return df_bow\n",
        "\n",
        "docList = ['the man went out for a walk', 'the children sat around the fire', 'Game of Thrones is an amazing tv series!', 'Game of Thrones is the best tv series!', 'Game of Thrones is so great']\n",
        "result = func_baf_of_words(docList, (3,3))\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "KDX_mLqjcDWU",
        "outputId": "feb19cde-366f-4234-b439-2c369fdd65f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-159b2c26-6741-4d50-b95b-b238e38995ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amazing</th>\n",
              "      <th>an</th>\n",
              "      <th>around</th>\n",
              "      <th>best</th>\n",
              "      <th>children</th>\n",
              "      <th>fire</th>\n",
              "      <th>for</th>\n",
              "      <th>game</th>\n",
              "      <th>great</th>\n",
              "      <th>is</th>\n",
              "      <th>...</th>\n",
              "      <th>of</th>\n",
              "      <th>out</th>\n",
              "      <th>sat</th>\n",
              "      <th>series</th>\n",
              "      <th>so</th>\n",
              "      <th>the</th>\n",
              "      <th>thrones</th>\n",
              "      <th>tv</th>\n",
              "      <th>walk</th>\n",
              "      <th>went</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.428411</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.428411</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.286912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.428411</td>\n",
              "      <td>0.428411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41544</td>\n",
              "      <td>0.41544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.556451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.442986</td>\n",
              "      <td>0.442986</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.296673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.296673</td>\n",
              "      <td>...</td>\n",
              "      <td>0.296673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.357398</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.296673</td>\n",
              "      <td>0.357398</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.469096</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314159</td>\n",
              "      <td>...</td>\n",
              "      <td>0.314159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.378464</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314159</td>\n",
              "      <td>0.314159</td>\n",
              "      <td>0.378464</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.343824</td>\n",
              "      <td>0.513391</td>\n",
              "      <td>0.343824</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.513391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.343824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-159b2c26-6741-4d50-b95b-b238e38995ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-159b2c26-6741-4d50-b95b-b238e38995ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-159b2c26-6741-4d50-b95b-b238e38995ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    amazing        an   around      best  children     fire       for  \\\n",
              "0  0.000000  0.000000  0.00000  0.000000   0.00000  0.00000  0.428411   \n",
              "1  0.000000  0.000000  0.41544  0.000000   0.41544  0.41544  0.000000   \n",
              "2  0.442986  0.442986  0.00000  0.000000   0.00000  0.00000  0.000000   \n",
              "3  0.000000  0.000000  0.00000  0.469096   0.00000  0.00000  0.000000   \n",
              "4  0.000000  0.000000  0.00000  0.000000   0.00000  0.00000  0.000000   \n",
              "\n",
              "       game     great        is  ...        of       out      sat    series  \\\n",
              "0  0.000000  0.000000  0.000000  ...  0.000000  0.428411  0.00000  0.000000   \n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.41544  0.000000   \n",
              "2  0.296673  0.000000  0.296673  ...  0.296673  0.000000  0.00000  0.357398   \n",
              "3  0.314159  0.000000  0.314159  ...  0.314159  0.000000  0.00000  0.378464   \n",
              "4  0.343824  0.513391  0.343824  ...  0.343824  0.000000  0.00000  0.000000   \n",
              "\n",
              "         so       the   thrones        tv      walk      went  \n",
              "0  0.000000  0.286912  0.000000  0.000000  0.428411  0.428411  \n",
              "1  0.000000  0.556451  0.000000  0.000000  0.000000  0.000000  \n",
              "2  0.000000  0.000000  0.296673  0.357398  0.000000  0.000000  \n",
              "3  0.000000  0.314159  0.314159  0.378464  0.000000  0.000000  \n",
              "4  0.513391  0.000000  0.343824  0.000000  0.000000  0.000000  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "####  TfidfVectorizer   ############\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def func_tfidf(docList):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectors = vectorizer.fit_transform(docList)\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "  dense = vectors.todense()\n",
        "  denselist = dense.tolist()\n",
        "  df = pd.DataFrame(denselist, columns=feature_names)\n",
        "  return df\n",
        "\n",
        "docList = ['the man went out for a walk', 'the children sat around the fire', 'Game of Thrones is an amazing tv series!', 'Game of Thrones is the best tv series!', 'Game of Thrones is so great']\n",
        "func_tfidf(docList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4SZOmb7cDXo",
        "outputId": "8c1ce4d9-96f4-4cd3-ba3c-bee9c5ec5c77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('just', 0.9997512698173523),\n",
              " ('yes', 0.9997451305389404),\n",
              " ('more', 0.999728262424469),\n",
              " ('with', 0.9997220635414124),\n",
              " ('get', 0.9997200965881348),\n",
              " ('games', 0.9997031688690186),\n",
              " ('by', 0.9996980428695679),\n",
              " ('until', 0.9996848106384277),\n",
              " ('liao', 0.9996798038482666),\n",
              " ('play', 0.999678373336792)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "########    Word2Vec Model   ##############\n",
        "\n",
        "import pandas as pd\n",
        "import gensim\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"SMSCollection.csv\")\n",
        "#df.head()\n",
        "\n",
        "\n",
        "def func_word_2_vec(df, column_name):\n",
        "  #Simple Preprocessing & Tokenization\n",
        "  df['review_text'] = df[column_name].apply(gensim.utils.simple_preprocess)\n",
        "\n",
        "  # Initialized the Word2Vec Model\n",
        "  model = gensim.models.Word2Vec(window=10, min_count=2, workers=4,)\n",
        "\n",
        "  # Build vocabulary \n",
        "  model.build_vocab(df['review_text'], progress_per=1000)\n",
        "\n",
        "  #Train the Word2Vec Model\n",
        "  model.train(df['review_text'], total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = func_word_2_vec(df, 'sms')\n",
        "\n",
        "# Finding Similar Words and Similarity between words\n",
        "model.wv.most_similar(\"crazy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRM_LTlkcDhq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMFvyQ-dcDls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNiklHjHcDnZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDfpCKsZcDqr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb07s_RkcDr8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlqh_0EYcDw1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmSZCYKNcDzH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YNQk_NZcD3Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5u5WzgdcD54"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}